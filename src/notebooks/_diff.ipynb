{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1cf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "root_path = Path(\"/home/olivieri/exp\").resolve()\n",
    "src_path = root_path / \"src\"\n",
    "sys.path.append(f\"{str(src_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cd63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed # for paralellism\n",
    "import time\n",
    "\n",
    "from prompter import *\n",
    "from data import *\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a2a09",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079bc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"gemma3:4b-it-q4_K_M\"\n",
    "# model_name = \"gemma3:4b-it-qat\"\n",
    "# model_name = \"gemma3:12b\"\n",
    "# model_name = \"gemma3:12b-it-qat\"\n",
    "# model_name = \"gemma3:27b\"\n",
    "# model_name = \"gemma3:27b-it-qat\"\n",
    "# model_name = \"qwen2.5vl:7b-q4_K_M\"\n",
    "# model_name = \"hf.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\"\n",
    "# model_name = \"qwen2.5vl:32b-q4_K_M\"\n",
    "\n",
    "model_name = \"gemma3:4b-it-qat\"\n",
    "\n",
    "vlm = OllamaMLLM(model_name)\n",
    "\n",
    "llm_judge: GoogleAIStudioMLLM = GoogleAIStudioMLLM(model_name=\"gemini-2.0-flash\", api_key=os.environ[\"GOOGLE_AI_KEY_1\"])\n",
    "\n",
    "# Setting\n",
    "BY_MODEL = \"LRASPP_MobileNet_V3\"\n",
    "SPLIT_BY = \"non-splitted\"\n",
    "\n",
    "promptBuilder = PromptBuilder(\n",
    "    by_model            = BY_MODEL,\n",
    "    alpha               = 0.7,\n",
    "    image_size          = 224,\n",
    "    array_size          = (32, 32),\n",
    "    class_map           = CLASS_MAP, # imported from 'class_map.py'\n",
    "    color_map           = COLOR_MAP_DICT,\n",
    "    split_by            = SPLIT_BY\n",
    ")\n",
    "\n",
    "gen_params = GenParams(seed=get_seed(), temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b60d9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367f229",
   "metadata": {},
   "source": [
    "### Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ed5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptBuilder.load_modules(\n",
    "    context_module          = ContextModule(variation=\"default\"),\n",
    "    color_map_module        = Names_ColorMapModule(variation=\"default\"),\n",
    "    input_format_module     = SepMasks_Ovr_InputFormatModule(\"original\"),\n",
    "    task_module             = TaskModule(variation=\"default\"),\n",
    "    output_format_module    = OutputFormatModule(variation=\"default\"),\n",
    "    support_set_module      = SupportSetModule(variation=\"default\", sup_set_idxs=()),\n",
    "    query_module            = QueryModule(variation=\"default\"),\n",
    "    eval_module             = EvalModule(variation=\"7_incomplet+strict+precision+error_types+spatial_locs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f9e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d33da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt = promptBuilder.build_inference_prompt(query_idx)\n",
    "# display_prompt(class_splitted_inference_prompt[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c358166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_idx': 22,\n",
       " 'content': 'The prediction mask captures the motorcycle region quite accurately, mirroring the ground truth almost perfectly. There are no significant deviations observed in the segmentation of the motorcycle.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_pr = await vlm.predict_one(\n",
    "    inference_prompt,\n",
    "    query_idx,\n",
    "    gen_params=gen_params,\n",
    "    only_text=True\n",
    ")\n",
    "answer_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55412eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = promptBuilder.build_eval_prompt(query_idx, answer_pr)\n",
    "len(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6555d806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_idx': 22,\n",
       " 'content': {'pred': 'incorrect',\n",
       "  'score': 1,\n",
       "  'reason': 'The predicted answer states that the MOTORBIKE region is captured quite accurately, which is the opposite of the ground truth. The ground truth mentions that the prediction mask is coarser, boundaries are less defined, and there are over-segmentation issues. The predicted answer also mentions fragmentation, but the overall assessment of accuracy is incorrect.'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pr = await llm_judge.predict_one(\n",
    "    eval_prompt,\n",
    "    query_idx,\n",
    "    gen_params=gen_params,\n",
    "    only_text=True,\n",
    "    parse_to_dict=True\n",
    ")\n",
    "eval_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d95f59",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41191a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_format = SepMasks_Ovr_InputFormatModule(\"original\")\n",
    "\n",
    "promptBuilder.load_modules(\n",
    "    context_module          = ContextModule(variation=\"default\"),\n",
    "    color_map_module        = Patches_ColorMapModule(variation=\"default\"),\n",
    "    input_format_module     = input_format,\n",
    "    task_module             = TaskModule(variation=\"default\"),\n",
    "    output_format_module    = OutputFormatModule(variation=\"default\"),\n",
    "    support_set_module      = SupportSetModule(variation=\"default\", sup_set_idxs=(16, 2, 18)),\n",
    "    query_module            = QueryModule(variation=\"default\"),\n",
    "    eval_module             = EvalModule(variation=\"7_incomplet+strict+precision+error_types+spatial_locs\")\n",
    ")\n",
    "\n",
    "prompt_desc = input_format.__class__.__name__.removesuffix(\"_InputFormatModule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550cb341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_idxs = [x for x in list(range(80)) if x not in promptBuilder.sup_set_idxs]\n",
    "len(epoch_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d3217",
   "metadata": {},
   "source": [
    "**Save Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432a7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_path = Path(\"/home/olivieri/exp/my_data/by_model/LRASPP_MobileNet_V3/non-splitted/answer_prs/gemini-2.0-flash/baseline/ConcatMasks_Ovr_Hz.jsonl\")\n",
    "eval_path = Path(\"/home/olivieri/exp/my_data/by_model/LRASPP_MobileNet_V3/non-splitted/eval_prs/gemini-2.0-flash/baseline/ConcatMasks_Ovr_Hz.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315af4df",
   "metadata": {},
   "source": [
    "### Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4724b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_prompts = [promptBuilder.build_inference_prompt(q_i) for q_i in epoch_idxs]\n",
    "len(inference_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0b713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_many_to_jsonl(answer_path, [{\"state\": promptBuilder.get_state()}]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9726582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ConcatMasks_Ovr_Hz.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;103;173;91m██████████\u001b[0m| 6/6 [05:16<00:00, 52.75s/item]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {prompt_desc}.\")\n",
    "\n",
    "epoch_pred_list = [{\"state\": promptBuilder.get_state()}]\n",
    "\n",
    "epoch_pred_list += await vlm.predict_many(\n",
    "    inference_prompts,\n",
    "    epoch_idxs,\n",
    "    gen_params=gen_params,\n",
    "    jsonl_save_path=answer_path,\n",
    "    only_text=True,\n",
    "    batch_size=13,\n",
    "    cooldown_period=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939fe2d2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f690e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_answer_list = get_many_eval_pr(answer_path, return_state=False, format_to_dict=False)\n",
    "len(epoch_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "099a536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;103;173;91m██████████\u001b[0m| 77/77 [00:00<00:00, 2805.70item/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"You are an intelligent chatbot designed for evaluating the correctness of AI assistant predictions for question-answer pairs.\\nYour task is to compare the predicted answer with the ground-truth answer and determine if the predicted answer is correct or not. Here's how you can accomplish the task:\\n\\n------\\n\\n## INSTRUCTIONS:\\n- Focus on the correctness, completeness and accuracy of the predicted answer with the ground-truth.\\n- Consider predictions with less or more specific details (as long as they show some consistency with the ground truth) as correct evaluation.\\n- Be strict with your evaluations.\\n- Expect precision from the predicted answer, it is not enough for it to roughly capture the essence of the ground truth, the prediction has to overlap sufficiently to the ground truth to be considered correct.\\n- Be critical of significant inconsistencies of error types and spatial locations.\\n\\nPlease evaluate the following answer:\\n\\nGround truth correct Answer:\\nThe ground truth AEROPLANE regions have been segmented in a coarser and incomplete way, especially regarding the wings, and two tiny AEROPLANE patches have been hallucinated on the right edge. The prediction mask for the ground truth PERSON region on the center-bottom-left is slightly more blob-like.\\n\\n\\nPredicted Answer:\\nThe ground truth AEROPLANE region has been segmented with somewhat coarser, less defined boundaries, and these tend to be slightly over-extended, especially on the right.\\n\\nProvide your evaluation as a correct/incorrect prediction along with the score where the score is an integer value between 0 (fully wrong) and 5 (fully correct). The middle score provides the percentage of correctness.\\nPlease generate the response in the form of a Python dictionary string with keys 'reason', 'score' and 'pred', where value of 'pred' is a string of 'correct' or 'incorrect', value of 'score' is in INTEGER, not STRING and value of 'reason' should provide the reason behind the decision.\\nOnly provide the Python dictionary string. Escape properly quotes within the 'reason' string.\\nFor example, your response should look like this: {'reason': reason, 'score': 4, 'pred': 'correct'}.\\n\"],\n",
       " [\"You are an intelligent chatbot designed for evaluating the correctness of AI assistant predictions for question-answer pairs.\\nYour task is to compare the predicted answer with the ground-truth answer and determine if the predicted answer is correct or not. Here's how you can accomplish the task:\\n\\n------\\n\\n## INSTRUCTIONS:\\n- Focus on the correctness, completeness and accuracy of the predicted answer with the ground-truth.\\n- Consider predictions with less or more specific details (as long as they show some consistency with the ground truth) as correct evaluation.\\n- Be strict with your evaluations.\\n- Expect precision from the predicted answer, it is not enough for it to roughly capture the essence of the ground truth, the prediction has to overlap sufficiently to the ground truth to be considered correct.\\n- Be critical of significant inconsistencies of error types and spatial locations.\\n\\nPlease evaluate the following answer:\\n\\nGround truth correct Answer:\\nThe ground truth AEROPLANE regions have been captured with irregular and erratic boundaries, while some AEROPLANE area has been hallucinated on the bottom of the scene.\\n\\n\\nPredicted Answer:\\nThe ground truth AEROPLANE region has been segmented with somewhat coarser, less defined boundaries, and these tend to be slightly over-extended, especially on the bottom right.\\n\\nProvide your evaluation as a correct/incorrect prediction along with the score where the score is an integer value between 0 (fully wrong) and 5 (fully correct). The middle score provides the percentage of correctness.\\nPlease generate the response in the form of a Python dictionary string with keys 'reason', 'score' and 'pred', where value of 'pred' is a string of 'correct' or 'incorrect', value of 'score' is in INTEGER, not STRING and value of 'reason' should provide the reason behind the decision.\\nOnly provide the Python dictionary string. Escape properly quotes within the 'reason' string.\\nFor example, your response should look like this: {'reason': reason, 'score': 4, 'pred': 'correct'}.\\n\"]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts = [promptBuilder.build_eval_prompt(img_idx, epoch_answer_list[i][\"content\"]) for i, img_idx in my_tqdm(epoch_idxs)]\n",
    "eval_prompts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fc607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_many_to_jsonl(eval_path, [{\"state\": promptBuilder.get_state()}]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "916e38a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ConcatMasks_Ovr_Hz.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;103;173;91m██████████\u001b[0m| 6/6 [05:09<00:00, 51.55s/item]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {prompt_desc}.\")\n",
    "\n",
    "epoch_eval_list = [{\"state\": promptBuilder.get_state()}]\n",
    "\n",
    "epoch_eval_list += await llm_judge.predict_many(\n",
    "    eval_prompts,\n",
    "    epoch_idxs,\n",
    "    gen_params=gen_params,\n",
    "    jsonl_save_path=eval_path,\n",
    "    only_text=True,\n",
    "    parse_to_dict=True,\n",
    "    batch_size=13,\n",
    "    cooldown_period=60\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
