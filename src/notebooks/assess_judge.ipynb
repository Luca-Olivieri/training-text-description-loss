{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba42ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "root_path = Path(\"/home/olivieri/exp\").resolve()\n",
    "src_path = root_path / \"src\"\n",
    "sys.path.append(f\"{str(src_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe241b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from data import *\n",
    "from path import get_eval_prs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22f81f",
   "metadata": {},
   "source": [
    "# Eval Prompt Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0af519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "BY_MODEL = \"LRASPP_MobileNet_V3\"\n",
    "IMAGE_RESIZING_MODE = \"letterboxed\"\n",
    "OUTPUT_MODE = \"points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71e72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_prs_path = get_selected_annots_path(BY_MODEL, IMAGE_RESIZING_MODE, OUTPUT_MODE) / \"eval_prs\" / \"llm_judge_assessment\"\n",
    "\n",
    "prs_path = glob(f\"{root_prs_path}/*.jsonl\")\n",
    "variations = sorted([os.path.splitext(os.path.basename(pr_p))[0] for pr_p in prs_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df676090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_gts = pd.DataFrame()\n",
    "df_score_gts = pd.DataFrame()\n",
    "df_pred_prs = pd.DataFrame()\n",
    "df_score_prs = pd.DataFrame()\n",
    "\n",
    "for var in variations:\n",
    "\n",
    "    eval_gts = get_many_eval_gt(BY_MODEL, IMAGE_RESIZING_MODE, OUTPUT_MODE, return_state=False)\n",
    "    eval_prs = get_many_eval_pr(BY_MODEL, IMAGE_RESIZING_MODE, OUTPUT_MODE, \"llm_judge_assessment\", var, return_state=False)\n",
    "    # eval_prs = get_many_item(pr_path, return_state=False)\n",
    "\n",
    "    _df_all_gts = pd.DataFrame.from_dict(eval_gts).transpose()\n",
    "    _df_pred_gts = _df_all_gts[\"pred\"].map(lambda x: x if x is None else x == \"correct\")\n",
    "    _df_score_gts = _df_all_gts[\"score\"]\n",
    "    \n",
    "    _df_all_prs = pd.DataFrame.from_dict(eval_prs).transpose()\n",
    "    _df_pred_prs = _df_all_prs[\"pred\"].map(lambda x: x if x is None else x == \"correct\")\n",
    "    _df_score_prs = _df_all_prs[\"score\"]\n",
    "\n",
    "    df_pred_gts = pd.concat([df_pred_gts, _df_pred_gts], axis=1)\n",
    "    df_score_gts = pd.concat([df_score_gts, _df_score_gts], axis=1)\n",
    "    \n",
    "    df_pred_prs = pd.concat([df_pred_prs, _df_pred_prs], axis=1)\n",
    "    df_score_prs = pd.concat([df_score_prs, _df_score_prs], axis=1)\n",
    "\n",
    "df_pred_prs.columns = variations\n",
    "df_score_prs.columns = variations\n",
    "df_pred_gts.columns = variations\n",
    "df_score_gts.columns = variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329f77",
   "metadata": {},
   "source": [
    "## Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefe9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_acc = df_pred_prs.eq(df_pred_gts).where(df_pred_prs.notna() & df_pred_gts.notna(), None).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353307eb",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad55e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_MAE = np.abs(df_score_gts - df_score_prs).mean(axis=0)\n",
    "score_ME = (df_score_gts - df_score_prs).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f6625",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f486b5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_acc.</th>\n",
       "      <th>score MAE</th>\n",
       "      <th>score ME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_original</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_incomplet</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_incomplet+spatial_locs</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_incomplet+lower_scores</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_incomplet+simplif</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_incomplet+lower_scores+simplif</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  pred_acc. score MAE score ME\n",
       "1_original                             0.75       1.1     -0.9\n",
       "2_incomplet                            0.90      0.95    -0.75\n",
       "3_incomplet+spatial_locs               0.85       1.0     -0.8\n",
       "4_incomplet+lower_scores               0.90      0.85    -0.65\n",
       "5_incomplet+simplif                    0.80      0.85    -0.45\n",
       "6_incomplet+lower_scores+simplif       0.80       0.8     -0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_table = pd.concat([pred_acc, score_MAE, score_ME], axis=1)\n",
    "score_table.columns = [\"pred_acc.\", \"score MAE\", \"score ME\"]\n",
    "score_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
