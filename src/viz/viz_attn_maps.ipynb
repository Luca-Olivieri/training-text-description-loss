{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b105f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df08fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.config import *\n",
    "\n",
    "from core.datasets import get_answer_objects, JsonlIO, VOC2012SegDataset\n",
    "from core.torch_utils import unprefix_state_dict\n",
    "from core.viz import overlay_map, create_diff_mask, write_html_multi_row_image_caption\n",
    "from core.prompter import get_significant_classes\n",
    "from core.torch_utils import blend_tensors\n",
    "from core.color_map import apply_colormap\n",
    "from core.data_utils import flatten_list_to_depth\n",
    "from models.vle import VLE_REGISTRY, VLEncoder, NewLayer, MapComputeMode\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "\n",
    "from core._types import Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decd225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = setup_config(BASE_CONFIG, Path('/home/olivieri/exp/src/viz/config.yml'))\n",
    "seg_config = config['seg']\n",
    "vle_config = config['vle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495069d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision-Language Encoder\n",
    "vle = VLE_REGISTRY.get(\n",
    "    name=\"flair\",\n",
    "    version='flair-cc3m-recap.pt',\n",
    "    pretrained_weights_root_path=vle_config['pretrained_weights_root_path'],\n",
    "    new_layers=vle_config['new_layers'],\n",
    "    device=config['device']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7897d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "vle_weights_path = Path('/home/olivieri/exp/data/private/exps/vle/1-dM_vs_dT/250902_2047/weights/flair-flair-cc3m-recap.pt-text_proj_L_b256_250902_2047.pth')\n",
    "if vle_weights_path.exists():\n",
    "    vle.model.load_state_dict(unprefix_state_dict(torch.load(vle_weights_path, map_location='cuda')['model_state_dict'], prefix='_orig_mod.'))\n",
    "else:\n",
    "    raise AttributeError(f\"ERROR: VLE weights path '{vle_weights_path}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddbbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idxs = [9, 10, 11, 12, 13, 14, 15, 17]\n",
    "\n",
    "map_compute_mode = MapComputeMode.ATTENTION\n",
    "map_resize_mode = TF.InterpolationMode.NEAREST\n",
    "normalize = True\n",
    "mask_alpha = 0.55 # the greater, the less scene is visible \n",
    "map_alpha = 1.0\n",
    "viz_image_size = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13286124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_maps_with_captions(\n",
    "        model: VLEncoder,\n",
    "        images: torch.Tensor,\n",
    "        captions: list[str],\n",
    "        map_compute_mode: MapComputeMode,\n",
    "        viz_image_size: Optional[int | list[int]] = None,\n",
    "        map_resize_mode: TF.InterpolationMode = TF.InterpolationMode.NEAREST,\n",
    "        normalize: bool = True,\n",
    ") -> list[tuple[Image.Image, str]]:\n",
    "    image_text_list = []\n",
    "\n",
    "    for img, text in zip(images, captions):\n",
    "        img_tensor = model.preprocess_images([img])\n",
    "        text_tensor = model.preprocess_texts([text])\n",
    "        sim = model.get_similarity(img_tensor, text_tensor, broadcast=False)\n",
    "        map, min_value, max_value = model.get_maps(\n",
    "            img_tensor, text_tensor,\n",
    "            map_compute_mode=map_compute_mode,\n",
    "            upsample_size=viz_image_size, upsample_mode=map_resize_mode,\n",
    "            attn_heads_idx=[0, 3, 5, 7] # as done by the authors\n",
    "        ) # [1, 1, H, W], m, M\n",
    "        map = map.squeeze(0) # [H, W]\n",
    "        if viz_image_size:\n",
    "            img = TF.resize(img, size=viz_image_size, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        ovr_img = overlay_map(img, map, normalize=normalize, alpha=map_alpha) # (H_viz, W_viz)\n",
    "\n",
    "        image_text_list.append((ovr_img, f\"SIM = {sim.item():.2f}\", f\"MIN VALUE = {min_value.item():.2f}, MAX VALUE = {max_value.item():.2f}\", text, \"---\"))\n",
    "\n",
    "    return image_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cs_pr_maps_with_captions(\n",
    "        img_idxs: list[int],\n",
    "        vle: VLEncoder,\n",
    "        mask_color: Literal['L', 'RB'],\n",
    "        alpha: float,\n",
    "        map_compute_mode: MapComputeMode,\n",
    "        map_resize_mode: TF.InterpolationMode,\n",
    "        viz_image_size: int | tuple[int, int],\n",
    "        normalize: bool,\n",
    "        contrastive: bool = False\n",
    ") -> list[str | Image.Image]:\n",
    "    prs_path = Path('/home/olivieri/exp/data/private/prompts_data/by_model/LRASPP_MobileNet_V3/class-splitted/answer_prs/gemma3:12b-it-qat/speed_test/SepMasks_Ovr_1fs.jsonl')\n",
    "    cs_answers_pr = get_answer_objects(prs_path, idxs=None, jsonlio=JsonlIO(), return_state=False, format_to_dict=True)\n",
    "    cs_answers_pr_text = [list(cs_answers_pr[i].values()) for i in img_idxs]\n",
    "\n",
    "    viz_ds = VOC2012SegDataset(\n",
    "        root_path=config['datasets']['VOC2012_root_path'],\n",
    "        split='prompts_split',\n",
    "        device=config['device'],\n",
    "        resize_size=config['seg']['image_size'],\n",
    "        center_crop=True,\n",
    "        img_idxs=img_idxs,\n",
    "        mask_prs_path=config['mask_prs_path']\n",
    "    )\n",
    "    \n",
    "    scs, gts, prs = viz_ds[:]\n",
    "\n",
    "    cs_ovr_mask_prs = []\n",
    "\n",
    "    for sc, gt, pr in zip(scs, gts, prs):\n",
    "        gt_sign_classes = get_significant_classes(gt)\n",
    "        pr_sign_classes = get_significant_classes(pr)\n",
    "        sign_classes = list(set(gt_sign_classes + pr_sign_classes))\n",
    "        if 0 in sign_classes and sign_classes != [0]:\n",
    "            sign_classes.remove(0)\n",
    "        sign_classes = sorted(sign_classes)\n",
    "\n",
    "        ovr_mask_prs = []\n",
    "\n",
    "        for pos_c in sign_classes:\n",
    "            pos_class_gt = (gt == pos_c)\n",
    "            pos_class_pr = (pr == pos_c)\n",
    "\n",
    "            diff_mask = create_diff_mask(pos_class_gt, pos_class_pr)\n",
    "\n",
    "            # L overlay image\n",
    "            ovr_diff_mask_L = blend_tensors(sc, diff_mask*255, alpha)\n",
    "\n",
    "            # RB overlay image\n",
    "            diff_mask += (diff_mask*pos_class_gt) #Â sets to 2 the false negatives\n",
    "            diff_mask_col_RB = apply_colormap([diff_mask], {0: (0, 0, 0), 1: (255, 0, 0), 2: (0, 0, 255)}).squeeze()\n",
    "            ovr_diff_mask_RB = blend_tensors(sc, diff_mask_col_RB, alpha)\n",
    "\n",
    "            if mask_color == 'L':\n",
    "                ovr_mask_prs.append(ovr_diff_mask_L)\n",
    "            elif mask_color == 'RB':\n",
    "                ovr_mask_prs.append(ovr_diff_mask_RB)\n",
    "\n",
    "        if contrastive:\n",
    "            ovr_mask_prs = [ovr_mask_prs[0]]*int(len(ovr_mask_prs)) # instead of show consider positive images, show only the first one.\n",
    "\n",
    "        cs_ovr_mask_prs.append(ovr_mask_prs)\n",
    "    \n",
    "    cs_pr_image_text_list = [compute_maps_with_captions(vle, ovr_mask_prs, answers_pr_text, map_compute_mode=map_compute_mode, viz_image_size=viz_image_size, map_resize_mode=map_resize_mode, normalize=normalize) for ovr_mask_prs, answers_pr_text in zip(cs_ovr_mask_prs, cs_answers_pr_text)]\n",
    "    \n",
    "    return cs_pr_image_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "269cbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'visual_proj_text_proj_l_full_b256'\n",
    "rows = dict()\n",
    "\n",
    "sim_datas = []\n",
    "\n",
    "for mask_a, contr in product(mask_alphas:=[mask_alpha, 0.0], contrs:=[False, True]):\n",
    "\n",
    "    if mask_a == 0.0 and contr == True:\n",
    "        continue\n",
    "\n",
    "    cs_pr_image_text_list = compute_cs_pr_maps_with_captions(\n",
    "        img_idxs=img_idxs,\n",
    "        vle=vle,\n",
    "        mask_color='L',\n",
    "        alpha=mask_a,\n",
    "        map_compute_mode=map_compute_mode,\n",
    "        map_resize_mode=map_resize_mode,\n",
    "        viz_image_size=viz_image_size,\n",
    "        normalize=normalize,\n",
    "        contrastive=contr\n",
    "    )\n",
    "    # display_prompt(flatten_list(cs_pr_image_text_list))\n",
    "\n",
    "    if sim_datas == []:\n",
    "        sim_datas = [\"\" for _ in range(len(flatten_list_to_depth(cs_pr_image_text_list, depth=1)))]\n",
    "\n",
    "    new_sim_data = [f\"{el[1]}<br>{el[2]}<br>\" for el in flatten_list_to_depth(cs_pr_image_text_list, depth=1)]\n",
    "    sim_datas = [s_d+new_s_d for s_d, new_s_d in zip(sim_datas, new_sim_data)]\n",
    "\n",
    "    images_row = [el[0] for el in flatten_list_to_depth(cs_pr_image_text_list, depth=1)]\n",
    "\n",
    "    rows |= {f\"{contr=}, {mask_a=}\": images_row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d94c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [s_d+f\"<br>{el[3]}\" for s_d, el in zip(sim_datas, flatten_list_to_depth(cs_pr_image_text_list, depth=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fd7c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created index.html. Open it in your browser to see the result.\n"
     ]
    }
   ],
   "source": [
    "write_html_multi_row_image_caption(title, rows, captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
