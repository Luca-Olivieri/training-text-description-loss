{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b767e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115ea6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from data import JSONLDataset, ImageDataset, ImageCaptionDataset, CLASS_MAP, get_image_UIDs, crop_image_preprocess_image_text_batch\n",
    "from path import SPLITS_PATH\n",
    "from models.vl_encoders import VLE_REGISTRY, VLEncoder\n",
    "from viz import print_layer_numel\n",
    "from utils import get_compute_capability\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.models import segmentation as segmodels\n",
    "from functools import partial\n",
    "from torchvision.transforms._presets import SemanticSegmentation\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4881807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vendors.flair.src.flair.train import backward, unwrap_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c8dbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2994fcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = ...\n",
    "\n",
    "image_ds = ImageDataset(Path('/home/olivieri/exp/data/data_gen/VOC2012/train_no_aug/images'))\n",
    "caption_ds = JSONLDataset(Path('/home/olivieri/exp/data/data_gen/VOC2012/train_no_aug/captions.jsonl'))\n",
    "\n",
    "image_caption_ds = ImageCaptionDataset(\n",
    "    image_ds,\n",
    "    caption_ds\n",
    ")\n",
    "len(image_caption_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb848038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual_proj.attn.in_proj_weight: 786,432\n",
      "visual_proj.attn.in_proj_bias : 1,536\n",
      "visual_proj.attn.out_proj.weight: 262,144\n",
      "visual_proj.attn.out_proj.bias: 512\n",
      "visual_proj.ln_q.weight       : 512\n",
      "visual_proj.ln_q.bias         : 512\n",
      "visual_proj.ln_k.weight       : 512\n",
      "visual_proj.ln_k.bias         : 512\n",
      "visual_proj.ln_v.weight       : 512\n",
      "visual_proj.ln_v.bias         : 512\n",
      "image_post.proj               : 393,216\n",
      "Total: 1,446,912\n"
     ]
    }
   ],
   "source": [
    "vle: VLEncoder = VLE_REGISTRY.get(\"flair\", device=CONFIG['device'])\n",
    "# vle.set_vision_trainable_params('visual_proj')\n",
    "vle.set_vision_trainable_params('mlp+visual_proj')\n",
    "print_layer_numel(vle.model, print_only_total=True, only_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc40d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_crop_fn = T.CenterCrop(CONFIG['vle']['image_size'])\n",
    "\n",
    "collate_fn = partial(\n",
    "    crop_image_preprocess_image_text_batch,\n",
    "    crop_fn=center_crop_fn,\n",
    "    preprocess_images_fn=vle.preprocess_images,\n",
    "    preprocess_texts_fn=vle.preprocess_texts\n",
    ")\n",
    "\n",
    "image_caption_dl = DataLoader(\n",
    "    image_caption_ds,\n",
    "    batch_size=CONFIG[\"vle\"]['train'][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    generator=TORCH_GEN.clone_state(),\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8475d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO investigate what this rank and world_size is.\n",
    "criterion = vle.create_loss(\n",
    "      add_mps_loss = True,\n",
    "      rank = 0,\n",
    "      world_size = 1,\n",
    "      num_caps_per_img = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf56d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = lambda n, p: p.ndim < 2 or \"bn\" in n or \"ln\" in n or \"bias\" in n or 'logit_scale' in n\n",
    "include = lambda n, p: not exclude(n, p)\n",
    "\n",
    "named_parameters = list(vle.model.named_parameters())\n",
    "gain_or_bias_params = [p for n, p in named_parameters if exclude(n, p) and p.requires_grad]\n",
    "rest_params = [p for n, p in named_parameters if include(n, p) and p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": gain_or_bias_params, \"weight_decay\": 0.},\n",
    "        {\"params\": rest_params, \"weight_decay\": 1e-2}, # authors use 0.5\n",
    "    ],\n",
    "    lr=5e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50/169, total_loss=tensor(9.3256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "step 100/169, total_loss=tensor(9.1323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "step 150/169, total_loss=tensor(9.1443, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, (images, texts) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_caption_dl):\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m         \u001b[38;5;66;03m# scs_img = (diff_img*255).to(torch.uint8) # for viewable images\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m         \u001b[38;5;66;03m# TODO handle AMP\u001b[39;00m\n\u001b[32m     15\u001b[39m         vle.model.train()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         vle_output = \u001b[43mvle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_and_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         optimizer.zero_grad()\n\u001b[32m     21\u001b[39m         losses = criterion(\n\u001b[32m     22\u001b[39m                 image_features=vle_output.global_image_token,\n\u001b[32m     23\u001b[39m                 image_tokens=vle_output.local_image_tokens.clone(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m                 output_dict=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     29\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/exp/src/models/vl_encoders.py:274\u001b[39m, in \u001b[36mFLAIRAdapter.encode_and_project\u001b[39m\u001b[34m(self, images, texts, broadcast)\u001b[39m\n\u001b[32m    271\u001b[39m global_text_token, local_text_tokens = \u001b[38;5;28mself\u001b[39m.model.encode_text(texts)        \u001b[38;5;66;03m# [B_t, d_t], [B_t, n_t, d_t]\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# project the raw embeddings into the same space\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m global_image_token: torch.Tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_post\u001b[49m(global_image_token)    \u001b[38;5;66;03m# [B_i, D]\u001b[39;00m\n\u001b[32m    275\u001b[39m local_image_tokens: torch.Tensor = \u001b[38;5;28mself\u001b[39m.model.image_post(local_image_tokens)    \u001b[38;5;66;03m# [B_i, n_i, D]\u001b[39;00m\n\u001b[32m    276\u001b[39m global_text_token: torch.Tensor = \u001b[38;5;28mself\u001b[39m.model.text_post(global_text_token)       \u001b[38;5;66;03m# [B_t, D]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:368\u001b[39m, in \u001b[36mOptimizedModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    365\u001b[39m         \u001b[38;5;66;03m# still initializing\u001b[39;00m\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33m_orig_mod\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules[\u001b[33m\"\u001b[39m\u001b[33m_orig_mod\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(vle.model.parameters(), lr=lr)\n",
    "\n",
    "if get_compute_capability() >= 7.0:\n",
    "    vle.model = torch.compile(vle.model)\n",
    "\n",
    "for epoch in range(CONFIG[\"vle\"]['train'][\"num_epochs\"]):\n",
    "\n",
    "    for step, (images, texts) in enumerate(image_caption_dl):\n",
    "            \n",
    "            # scs_img = (diff_img*255).to(torch.uint8) # for viewable images\n",
    "\n",
    "            # TODO handle AMP\n",
    "\n",
    "            vle.model.train()\n",
    "\n",
    "            vle_output = vle.encode_and_project(images, texts, broadcast=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            losses = criterion(\n",
    "                    image_features=vle_output.global_image_token,\n",
    "                    image_tokens=vle_output.local_image_tokens.clone(),\n",
    "                    text_features=vle_output.global_text_token.squeeze(1),\n",
    "                    logit_scale=vle.model.logit_scale,\n",
    "                    visual_proj=vle.model.visual_proj,\n",
    "                    logit_bias=vle.model.logit_bias,\n",
    "                    output_dict=True\n",
    "            )\n",
    "            total_loss = sum(losses.values())\n",
    "\n",
    "            scaler = None # for AMP\n",
    "            backward(total_loss, scaler)\n",
    "\n",
    "            grad_clip_norm = None\n",
    "            if grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(vle.model.parameters(), grad_clip_norm, norm_type=2.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if (step+1) % 50 == 0:\n",
    "                print(f\"step {step+1}/{len(image_caption_dl)}, {total_loss=}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                unwrap_model(vle.model).logit_scale.clamp_(0, math.log(100))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['vle']['train']['num_epochs']}, {total_loss=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
