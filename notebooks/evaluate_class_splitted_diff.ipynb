{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1cf190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import _nb_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cd63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed #Â for paralellism\n",
    "import time\n",
    "import re\n",
    "\n",
    "from prompter import *\n",
    "from data import *\n",
    "from utils import *\n",
    "from model import GenParams, GoogleAIStudioMLLM, OllamaMLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a2a09",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079bc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"gemma3:4b-it-q4_K_M\"\n",
    "# model_name = \"gemma3:4b-it-qat\"\n",
    "# model_name = \"gemma3:12b\"\n",
    "# model_name = \"gemma3:12b-it-qat\"\n",
    "# model_name = \"gemma3:27b\"\n",
    "# model_name = \"gemma3:27b-it-qat\"\n",
    "# model_name = \"qwen2.5vl:7b-q4_K_M\"\n",
    "# model_name = \"qwen2.5vl:7b-q8_0\"\n",
    "# model_name = \"hf.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q4_K_M\"\n",
    "\n",
    "model_name = \"qwen2.5vl:7b-q8_0\"\n",
    "vlm = OllamaMLLM(model_name)\n",
    "# vlm: GoogleAIStudioMLLM = GoogleAIStudioMLLM(model_name=\"gemma-3-27b-it\", api_key=GOOGLE_AI_KEY)\n",
    "\n",
    "llm_judge: GoogleAIStudioMLLM = GoogleAIStudioMLLM(model_name=\"gemini-2.0-flash\", api_key=GOOGLE_AI_KEY)\n",
    "\n",
    "# Setting\n",
    "BY_MODEL = \"LRASPP_MobileNet_V3\"\n",
    "SPLIT_BY = \"class-splitted\"\n",
    "\n",
    "promptBuilder = PromptBuilder(\n",
    "    by_model            = BY_MODEL,\n",
    "    alpha               = 0.75,\n",
    "    image_size          = 224,\n",
    "    array_size          = (32, 32),\n",
    "    class_map           = CLASS_MAP, # imported from 'class_map.py'\n",
    "    color_map           = COLOR_MAP_DICT,\n",
    "    split_by            = SPLIT_BY\n",
    ")\n",
    "\n",
    "gen_params = GenParams(seed=CONFIG[\"seed\"], temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66204944",
   "metadata": {},
   "source": [
    "## Â Class-splitted Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad913abf",
   "metadata": {},
   "source": [
    "## Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29108b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptBuilder.load_modules(\n",
    "    context_module          = ContextModule(variation=\"default\"),\n",
    "    color_map_module        = ClassSplitted_ColorMapModule(variation=\"default\"),\n",
    "    input_format_module     = SepMasks_Ovr_InputFormatModule(\"original\"),\n",
    "    task_module             = TaskModule(variation=\"default\"),\n",
    "    output_format_module    = OutputFormatModule(variation=\"default\"),\n",
    "    support_set_module      = SupportSetModule(variation=\"default\", sup_set_idxs=()),\n",
    "    query_module            = QueryModule(variation=\"default\"),\n",
    "    eval_module             = EvalModule(variation=\"4_specify_pos_class_recency_2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbab9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb42e54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([9, 12, 18])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_splitted_inference_prompt = promptBuilder.build_class_splitted_inference_prompts(query_idx)\n",
    "class_splitted_inference_prompt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36baffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_prompt(class_splitted_inference_prompt[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61c2762",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m class_splitted_answer_pr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m vlm\u001b[38;5;241m.\u001b[39mpredict_one_class_splitted(\n\u001b[1;32m      2\u001b[0m     class_splitted_inference_prompt,\n\u001b[1;32m      3\u001b[0m     query_idx,\n\u001b[1;32m      4\u001b[0m     gen_params\u001b[38;5;241m=\u001b[39mgen_params,\n\u001b[1;32m      5\u001b[0m     only_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     splits_in_parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m class_splitted_answer_pr\n",
      "File \u001b[0;32m~/exp/src/model.py:242\u001b[0m, in \u001b[0;36mMLLM.predict_one_class_splitted\u001b[0;34m(self, class_splitted_query_prompt, query_idx, gen_params, system_prompt, only_text, parse_to_dict, splits_in_parallel)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c, q_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(significant_classes, query_prompts):\n\u001b[0;32m--> 242\u001b[0m         class_splitted_answer_pr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_one(\n\u001b[1;32m    243\u001b[0m             q_p,\n\u001b[1;32m    244\u001b[0m             query_idx\u001b[38;5;241m=\u001b[39mquery_idx,\n\u001b[1;32m    245\u001b[0m             gen_params\u001b[38;5;241m=\u001b[39mgen_params,\n\u001b[1;32m    246\u001b[0m             system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m    247\u001b[0m             only_text\u001b[38;5;241m=\u001b[39monly_text,\n\u001b[1;32m    248\u001b[0m             parse_to_dict\u001b[38;5;241m=\u001b[39mparse_to_dict\n\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    251\u001b[0m         outs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate({c: class_splitted_answer_pr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/exp/src/model.py:123\u001b[0m, in \u001b[0;36mMLLM.predict_one\u001b[0;34m(self, query_prompt, query_idx, gen_params, system_prompt, only_text, parse_to_dict)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_one\u001b[39m(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    113\u001b[0m         query_prompt: Prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m         parse_to_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, Any]:\n\u001b[1;32m    121\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_prompt_to_conv(query_prompt, system_prompt)\n\u001b[0;32m--> 123\u001b[0m     answer_response: GenericResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    124\u001b[0m         conv,\n\u001b[1;32m    125\u001b[0m         gen_params\u001b[38;5;241m=\u001b[39mgen_params,\n\u001b[1;32m    126\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     out: Response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_response(answer_response)\n\u001b[1;32m    130\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response(out, only_text\u001b[38;5;241m=\u001b[39monly_text, parse_to_dict\u001b[38;5;241m=\u001b[39mparse_to_dict)\n",
      "File \u001b[0;32m~/exp/src/model.py:457\u001b[0m, in \u001b[0;36mOllamaMLLM.generate_response\u001b[0;34m(self, conversation, gen_params, stream)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    452\u001b[0m         conversation: Conversation,\n\u001b[1;32m    453\u001b[0m         gen_params: GenParams,\n\u001b[1;32m    454\u001b[0m         stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ollama\u001b[38;5;241m.\u001b[39mChatResponse \u001b[38;5;241m|\u001b[39m Generator[ollama\u001b[38;5;241m.\u001b[39mChatResponse, \u001b[38;5;28;01mNone\u001b[39;00m, ollama\u001b[38;5;241m.\u001b[39mChatResponse] \u001b[38;5;241m|\u001b[39m AsyncGenerator[ollama\u001b[38;5;241m.\u001b[39mChatResponse, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m--> 457\u001b[0m     response_or_generator: ollama\u001b[38;5;241m.\u001b[39mChatResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    458\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \n\u001b[1;32m    459\u001b[0m         messages\u001b[38;5;241m=\u001b[39mconversation,\n\u001b[1;32m    460\u001b[0m         options\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctx_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] ,\n\u001b[1;32m    462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeat_last_n\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_n_not_to_repeat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeat_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeat_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_predict\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    468\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    470\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: gen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_p\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    471\u001b[0m         },\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mgen_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_format\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    473\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream\n\u001b[1;32m    474\u001b[0m     ) \u001b[38;5;66;03m# generate response or generator of responses\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_or_generator\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ollama/_client.py:854\u001b[0m, in \u001b[0;36mAsyncClient.chat\u001b[0;34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    810\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    819\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001b[1;32m    820\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    855\u001b[0m     ChatResponse,\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    858\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[1;32m    859\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    860\u001b[0m       messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[1;32m    861\u001b[0m       tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[1;32m    862\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    863\u001b[0m       think\u001b[38;5;241m=\u001b[39mthink,\n\u001b[1;32m    864\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    865\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    866\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[1;32m    867\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    868\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    869\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ollama/_client.py:692\u001b[0m, in \u001b[0;36mAsyncClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    690\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ollama/_client.py:632\u001b[0m, in \u001b[0;36mAsyncClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    633\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1540\u001b[0m, in \u001b[0;36mAsyncClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1527\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m   1528\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   1529\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1539\u001b[0m )\n\u001b[0;32m-> 1540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m   1627\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1629\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1630\u001b[0m     request,\n\u001b[1;32m   1631\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1632\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1633\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1634\u001b[0m )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1654\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auth_flow\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1657\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1658\u001b[0m         request,\n\u001b[1;32m   1659\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1660\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1661\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1694\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m     )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1730\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1733\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    382\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    383\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 394\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    399\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    400\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    401\u001b[0m     stream\u001b[38;5;241m=\u001b[39mAsyncResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    402\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    403\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:136\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:106\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:177\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_backends/anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1254\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[1;32m   1252\u001b[0m ):\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1254\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/locks.py:213\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_splitted_answer_pr = await vlm.predict_one_class_splitted(\n",
    "    class_splitted_inference_prompt,\n",
    "    query_idx,\n",
    "    gen_params=gen_params,\n",
    "    only_text=True,\n",
    "    splits_in_parallel=False,\n",
    ")\n",
    "class_splitted_answer_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc2d802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! ðŸ‘‹ I'm Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind. The Gemma team are my creators. \\n\\nI'm widely available to the public â€“ I'm an *open weights* model, which means I'm pretty accessible! I take text *and* images as inputs and generate text as output. Basically, you can give me a prompt (text or an image!), and I'll do my best to give you a helpful and informative response.\\n\\nI don't have access to tools, real-time information or Google search.\\n\\nYou can find more information about me here: [https://ai.google.dev/gemma](https://ai.google.dev/gemma)\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await vlm.generate_response([\"Hi, who are you?\"], gen_params=gen_params, stream=True)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34862fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([9, 12, 18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_splitted_eval_prompt = promptBuilder.build_class_splitted_eval_prompt(query_idx, class_splitted_answer_pr[\"content\"])\n",
    "class_splitted_eval_prompt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d14659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_prompt(class_splitted_eval_prompt[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a132142d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_idx': 5,\n",
       " 'content': {9: {'pred': 'incorrect',\n",
       "   'score': 1,\n",
       "   'reason': 'The predicted answer focuses on the CHAIR class, but it inaccurately describes the quality of the prediction mask. The ground truth states that the CHAIR region segmentation is irregular, erratic, and has inaccurate boundaries, while the predicted answer claims the mask closely matches the ground truth overall and only mentions minor jagged edges and over-segmentation. This is a significant inconsistency.'},\n",
       "  12: {'pred': 'correct',\n",
       "   'score': 4,\n",
       "   'reason': \"The predicted answer accurately identifies several key deviations in the DOG class segmentation compared to the ground truth. It correctly points out the misclassification of non-white (black) regions of the dog as unlabelled, the incomplete segmentation of white regions, and the overall deviation from the ground truth. While it doesn't capture every single detail, it provides a comprehensive and accurate assessment of the DOG class segmentation errors.\"},\n",
       "  18: {'pred': 'incorrect',\n",
       "   'score': 1,\n",
       "   'reason': 'The predicted answer focuses on the SOFA class, noting misclassifications and boundary inaccuracies. However, the ground truth does not mention the SOFA class at all, implying that there is no SOFA in either mask. Therefore, the predicted answer deviates significantly from the ground truth, making it incorrect.'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_splitted_eval_pr = await llm_judge.predict_one_class_splitted(\n",
    "    class_splitted_eval_prompt,\n",
    "    query_idx,\n",
    "    gen_params=gen_params,\n",
    "    only_text=True,\n",
    "    parse_to_dict=True,\n",
    "    splits_in_parallel=True\n",
    ")\n",
    "class_splitted_eval_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b0edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pertinence(class_splitted_answer_pr[\"content\"].values(), class_splitted_answer_pr[\"content\"].keys())\n",
    "validate_pertinence([d[\"reason\"] for d in class_splitted_eval_pr[\"content\"].values()], class_splitted_eval_pr[\"content\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad970c",
   "metadata": {},
   "source": [
    "## Pertinence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_format = ConcatMasks_Ovr_Hz_InputFormatModule(\"original\")\n",
    "\n",
    "promptBuilder.load_modules(\n",
    "    context_module          = ContextModule(variation=\"default\"),\n",
    "    color_map_module        = ClassSplitted_ColorMapModule(variation=\"default\"),\n",
    "    input_format_module     = input_format,\n",
    "    task_module             = TaskModule(variation=\"default\"),\n",
    "    output_format_module    = OutputFormatModule(variation=\"default\"),\n",
    "    support_set_module      = SupportSetModule(variation=\"default\", sup_set_idxs=(16, 2, 18)),\n",
    "    query_module            = QueryModule(variation=\"default\"),\n",
    "    eval_module             = EvalModule(variation=\"4_specify_pos_class_recency_2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fcd4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partition_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epoch_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m23\u001b[39m))\n\u001b[1;32m      2\u001b[0m epoch_idxs \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m epoch_idxs \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m promptBuilder\u001b[38;5;241m.\u001b[39msup_set_idxs]\n\u001b[0;32m----> 3\u001b[0m batches_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mpartition_list\u001b[49m(epoch_idxs, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      4\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batches_idxs)\n\u001b[1;32m      5\u001b[0m batches_idxs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partition_list' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_idxs = list(range(23))\n",
    "epoch_idxs = [x for x in epoch_idxs if x not in promptBuilder.sup_set_idxs]\n",
    "batches_idxs = partition_list(epoch_idxs, 7)\n",
    "num_batches = len(batches_idxs)\n",
    "batches_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64fa15",
   "metadata": {},
   "source": [
    "### Validate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ConcatMasks_Ovr_Hz.\n",
      "Batch 1/3 completed.\n",
      "Batch 2/3 completed.\n",
      "Batch 3/3 completed.\n"
     ]
    }
   ],
   "source": [
    "epoch_answer_pr_dicts = {}\n",
    "prompt_desc = input_format.__class__.__name__.removesuffix(\"_InputFormatModule\")\n",
    "print(f\"Evaluating {prompt_desc}.\")\n",
    " \n",
    "def process_class_splitted_predict(promptBuilder, query_idx):\n",
    "    return query_idx, vlm.class_splitted_predict_one(promptBuilder, query_idx)\n",
    "\n",
    "for i, batch_idxs in enumerate(batches_idxs):\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust workers as needed\n",
    "        future_to_idx = {executor.submit(lambda idx: process_class_splitted_predict(promptBuilder, idx), query_idx): query_idx for query_idx in batch_idxs}\n",
    "        for future in as_completed(future_to_idx):\n",
    "            query_idx, answer_pr_dict = future.result()\n",
    "            epoch_answer_pr_dicts[query_idx] = answer_pr_dict\n",
    "\n",
    "    print(f\"Batch {i+1}/{num_batches} completed.\")\n",
    "\n",
    "    time.sleep(60) if i < num_batches - 1 else None  # Sleep only if not the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx, pos_class_2_answer_pr in epoch_answer_pr_dicts.items():\n",
    "    validate_pertinence(pos_class_2_answer_pr.values(), pos_class_2_answer_pr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80670257",
   "metadata": {},
   "source": [
    "### Validate Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61748da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ConcatMasks_Ovr_Hz.\n",
      "Batch 1/3 completed.\n",
      "Batch 2/3 completed.\n",
      "Batch 3/3 completed.\n"
     ]
    }
   ],
   "source": [
    "epoch_eval_pr_dicts = {}\n",
    "prompt_desc = input_format.__class__.__name__.removesuffix(\"_InputFormatModule\")\n",
    "print(f\"Evaluating {prompt_desc}.\")\n",
    "\n",
    "def process_class_splitted_evaluate(promptBuilder, query_idx):\n",
    "    pos_class_2_eval_prompt = promptBuilder.build_class_splitted_eval_prompt(query_idx, epoch_answer_pr_dicts[query_idx])\n",
    "    return query_idx, vlm.class_splitted_evaluate_one(pos_class_2_eval_prompt)\n",
    "\n",
    "for i, batch_idxs in enumerate(batches_idxs):\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:  # Adjust workers as needed\n",
    "        future_to_idx = {executor.submit(lambda idx: process_class_splitted_evaluate(promptBuilder, idx), query_idx): query_idx for query_idx in batch_idxs}\n",
    "        for future in as_completed(future_to_idx):\n",
    "            query_idx, eval_pr_dict = future.result()\n",
    "            epoch_eval_pr_dicts[query_idx] = eval_pr_dict\n",
    "\n",
    "    print(f\"Batch {i+1}/{num_batches} completed.\")\n",
    "\n",
    "    time.sleep(60) if i < num_batches - 1 else None  # Sleep only if not the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_eval_idxs = []\n",
    "n_evals = 0\n",
    "for img_idx, pos_class_2_eval_pr in epoch_eval_pr_dicts.items():\n",
    "    eval_reasons = [eval_pr[\"reason\"] for eval_pr in pos_class_2_eval_pr.values()]\n",
    "    n_evals += len(eval_reasons)\n",
    "    try:\n",
    "        validate_pertinence(eval_reasons, pos_class_2_eval_pr.keys())\n",
    "    except:\n",
    "        wrong_eval_idxs.append(img_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9145a077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_eval_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bdbab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wrong_idx \u001b[38;5;241m=\u001b[39m \u001b[43mwrong_eval_idxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m wrong_idx\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "wrong_idx = wrong_eval_idxs[0]\n",
    "wrong_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH ANSWER:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: 'The prediction masks for the two ground truth TRAIN regions have some coarser boundaries, many details are lost.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED ANSWER:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{15: 'The prediction mask does not contain any PERSON class, while the ground truth mask does not contain any PERSON class either. Therefore, there are no significant deviations.\\n',\n",
       " 19: 'The prediction mask of the TRAIN class is incomplete, especially in the lower part of the scene. The TRAIN region on the right side of the scene is also missing some parts. The boundaries of the predicted TRAIN regions are also less defined and more irregular compared to the ground truth.\\n'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED EVAL:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{15: 'The predicted answer states that both the prediction and ground truth masks do not contain the PERSON class, and therefore there are no significant deviations. This aligns with the ground truth\\'s implicit statement that \"Both masks have no PERSON in them, so there is no deviation\".',\n",
       " 19: 'The predicted answer correctly identifies that the prediction mask of the TRAIN class is incomplete and that the boundaries are less defined and more irregular compared to the ground truth. The predicted answer is consistent with the ground truth answer.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"GROUND TRUTH ANSWER:\")\n",
    "display(get_one_answer_gt(BY_MODEL, wrong_idx, False))\n",
    "print(\"PREDICTED ANSWER:\")\n",
    "display(epoch_answer_pr_dicts[wrong_idx])\n",
    "print(\"PREDICTED EVAL:\")\n",
    "display({k: v[\"reason\"] for k, v in epoch_eval_pr_dicts[wrong_idx].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878666",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba19d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_format = SepMasks_Ovr_InputFormatModule(\"original\")\n",
    "\n",
    "promptBuilder.load_modules(\n",
    "    context_module          = ContextModule(variation=\"default\"),\n",
    "    color_map_module        = ClassSplitted_ColorMapModule(variation=\"default\"),\n",
    "    input_format_module     = input_format,\n",
    "    task_module             = TaskModule(variation=\"default\"),\n",
    "    output_format_module    = OutputFormatModule(variation=\"default\"),\n",
    "    support_set_module      = SupportSetModule(variation=\"default\", sup_set_idxs=(16,)),\n",
    "    query_module            = QueryModule(variation=\"default\"),\n",
    "    eval_module             = EvalModule(variation=\"4_specify_pos_class_recency_2\")\n",
    ")\n",
    "\n",
    "prompt_desc = input_format.__class__.__name__.removesuffix(\"_InputFormatModule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21338ad",
   "metadata": {},
   "source": [
    "**Save Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b028917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"speed_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79896966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/olivieri/exp/my_data/by_model/LRASPP_MobileNet_V3/class-splitted/answer_prs/qwen2.5vl:7b-q8_0/speed_test/SepMasks_Ovr_1fs.jsonl\n",
      "/home/olivieri/exp/my_data/by_model/LRASPP_MobileNet_V3/class-splitted/eval_prs/qwen2.5vl:7b-q8_0/speed_test/SepMasks_Ovr_1fs.jsonl\n"
     ]
    }
   ],
   "source": [
    "answer_path = get_answer_prs_root_path(\"LRASPP_MobileNet_V3\", \"class-splitted\") / model_name / exp_name / \"SepMasks_Ovr_1fs.jsonl\"\n",
    "eval_path = get_eval_prs_root_path(\"LRASPP_MobileNet_V3\", \"class-splitted\") / model_name / exp_name / \"SepMasks_Ovr_1fs.jsonl\"\n",
    "print(answer_path)\n",
    "print(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0753f69",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8004a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_answer_list, answer_state = get_many_eval_pr(answer_path, return_state=True, format_to_dict=False)\n",
    "len(epoch_answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5598fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_idxs = [d[\"img_idx\"] for d in epoch_answer_list]\n",
    "len(epoch_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3801b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;103;173;91mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 79/79 [00:00<00:00, 1020.35item/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_splitted_eval_prompts = [promptBuilder.build_class_splitted_eval_prompt(img_idx, epoch_answer_list[i][\"content\"]) for i, img_idx in my_tqdm(epoch_idxs)]\n",
    "len(class_splitted_eval_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bab329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_many_to_jsonl(eval_path, [answer_state | {\"llm_judge\": f\"{llm_judge.__class__.__name__}:{llm_judge.model}\"}]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SepMasks_Ovr.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;103;173;91mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 4/4 [03:05<00:00, 46.42s/item]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating {prompt_desc}.\")\n",
    "\n",
    "epoch_eval_list = []\n",
    "\n",
    "epoch_eval_list += await llm_judge.predict_many_class_splitted(\n",
    "    class_splitted_eval_prompts,\n",
    "    epoch_idxs,\n",
    "    gen_params=gen_params,\n",
    "    jsonl_save_path=eval_path,\n",
    "    only_text=True,\n",
    "    parse_to_dict=True,\n",
    "    splits_in_parallel=True,\n",
    "    batch_size=11,\n",
    "    cooldown_period=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27e621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
