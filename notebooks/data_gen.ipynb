{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3ea2c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import _nb_utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eeee88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import CONFIG\n",
    "from prompter import *\n",
    "from data import *\n",
    "from utils import *\n",
    "from model import GenParams, GoogleAIStudioMLLM, OllamaMLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7df73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma3:4b-it-qat\"\n",
    "\n",
    "llm = OllamaMLLM(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb23e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "BY_MODEL = \"LRASPP_MobileNet_V3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4350f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params = GenParams(\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32624063",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_blueprint={\n",
    "        \"role\": \"0_baseline\",\n",
    "        \"instruct\": \"0_baseline\",\n",
    "        \"output_format\": \"0_baseline\",\n",
    "        \"seeds_intro\": \"0_baseline\",\n",
    "        \"seeds\": \"0_baseline\",\n",
    "        \"query\": \"0_baseline\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0050ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_level_range = [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab259139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_prompt_builder = DataGenPromptBuilder(\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    prompt_blueprint=prompt_blueprint,\n",
    "    by_model=BY_MODEL,\n",
    "    seed_idxs=range(0, 10),\n",
    "    score_level_range=[1, 5],\n",
    "    num_seeds=5,\n",
    "    num_outputs=4,\n",
    "    rotate_prompts=False,\n",
    "    jsonl_save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666ec93",
   "metadata": {},
   "source": [
    "## Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "030ca6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_idxs = [0, 1, 2, 4, 5]\n",
    "query_idxs = [0, 1, 2, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74937caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in evaluating the quality of textual descriptions of predictions produced against ground truth.\\n',\n",
       " \"Your task is to generate synthetic data <text, score> pairs, in which :\\n- 'text' describes how well a prediction aligns with a ground truth.\\n- 'score' (integer in the range [1, 5], the higher the better) indicating the quality of that prediction based on 'text'.\\n\",\n",
       " \"Your output should be a JSON array of objects containing the 'text' and 'score' field.\",\n",
       " 'Here are a few examples of the desired output:\\n',\n",
       " \"{0: 'The ground truth AEROPLANE regions have been segmented in a coarser and incomplete way, especially regarding the wings, and two tiny AEROPLANE patches have been hallucinated on the right edge. The prediction mask for the ground truth PERSON region on the center-bottom-left is slightly more blob-like.\\\\n'}\",\n",
       " '\\n',\n",
       " \"{1: 'The ground truth AEROPLANE regions have been captured with irregular and erratic boundaries, while some AEROPLANE area has been hallucinated on the bottom of the scene.\\\\n'}\",\n",
       " '\\n',\n",
       " \"{2: 'The ground truth MONITOR region on the center of the scene has been segmented with somewhat coarser and imperfect boundaries. The bottom and right boundaries of the region are especially flawed.\\\\n'}\",\n",
       " '\\n',\n",
       " \"{4: 'The prediction masks for the ground truth BOAT regions are coarser, imprecise and imperfect: the region on the right has been over-segmented a bit, while the region on the center has been slightly under-segmented and the vertical stripe on the right has mostly been missed.\\\\n'}\",\n",
       " '\\n',\n",
       " \"{5: 'The prediction has many deviations from the ground truth and is overall much more chaotic. The prediction mask of the ground truth DOG region on the center is severily incomplete and irregular. The CHAIR region taking most of the scene has been segmented in a more irregular and erratic way, and the boundaries are inaccurate. Some area have been misclassified as SOFA instead of CHAIR on the top edge, and some has been misclassified as CHAIR instead of DOG in the center.\\\\n'}\",\n",
       " '\\n',\n",
       " 'Please generate 5 unique <text, score> pairs. Ensure variety in the score level, sentence structure, vocabulary, and the specific aspects of similarity/difference. Avoid to repeat the examples provided.\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = [str(get_one_answer_gt(by_model=BY_MODEL, idx=i, return_state=False)) for i in seed_idxs] \n",
    "data_gen_prompt_builder.build_data_gen_prompt(prompt_blueprint, seeds=seeds, num_outputs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033cd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong parsing to dict!\n",
      "{'img_idx': [0, 1, 2, 4, 5], 'content': '{\\n    \"0\": \"The predicted bounding box for the ground truth BUILDING region is noticeably larger than the actual area, and the surrounding context is incorrectly labeled as ROAD. The edges of the predicted region appear somewhat diffuse, particularly at the top.\\\\n\"\\n  },\\n  {\\n    \"1\": \"While the prediction mostly aligns with the ground truth VEHICLE region, there\\'s a small but significant area of the predicted mask that extends beyond the true boundary, creating an \\'over-prediction\\'. The color segmentation is generally accurate, but some minor misclassifications of small elements as BACKGROUND exist.\\\\n\"\\n  },\\n  {\\n    \"2\": \"The ground truth PLANT region\\'s boundaries are sharply defined, but the prediction contains a substantial amount of false positives â€“ specifically, regions of the background have been incorrectly labeled as PLANT. The prediction mask is significantly less detailed and has a higher level of noise compared to the ground truth.\\\\n\"\\n  },\\n  {\\n    \"3\": \"The predicted segmentation of the ground truth HUMAN region shows considerable inconsistencies. There\\'s a large area of missing pixels, and several erroneous patches have been labeled as FLOOR instead of HUMAN.  The general shape of the predicted region is very different from the ground truth, indicating a substantial misalignment.\\\\n\"\\n  }'}\n"
     ]
    }
   ],
   "source": [
    "await data_gen_prompt_builder.generate_one_sample(\n",
    "    model=llm,\n",
    "    gen_params=gen_params,\n",
    "    query_idxs=query_idxs,\n",
    "    seed_idxs=seed_idxs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
